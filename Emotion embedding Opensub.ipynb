{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Final Model good Opensub.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"id":"vgyQxzatV2Vy"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFl8SHvJTGRY"},"source":["!pip install py-rouge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UnJNAx_WEVa","executionInfo":{"status":"ok","timestamp":1601479690182,"user_tz":-120,"elapsed":35701,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"4ad3b384-ada8-4b0b-c47c-9c544d98a7b5","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40GiOpWvFEyV"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"OPLXraeBE4xY","executionInfo":{"status":"ok","timestamp":1601479697195,"user_tz":-120,"elapsed":42698,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"6b2ed126-d465-499c-eedd-af5655127bd4","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import Data_processing as dpros\n","import Emotion_model as EM\n","import test_functions as test_func\n","import statistics\n","from tqdm import tqdm\n","from torch.autograd import Variable\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import pytorch_lightning as pl\n","import nltk\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch.utils.data as utils\n","from torch.utils.data import Dataset, DataLoader\n","import nltk.translate.bleu_score as bleu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYASPjA0RFrG"},"source":["# Automatic Dialogue Generation with Expressed Emotions"]},{"cell_type":"markdown","metadata":{"id":"nz5vPHFo-Hwi"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"dWO-FlEnX-QG"},"source":["\n","file_source_1 = 'drive/My Drive/Colab Notebooks/source_train_word.txt'\n","file_target_1 = 'drive/My Drive/Colab Notebooks/target_train_emotion.txt'\n","\n","file_source_2 = 'drive/My Drive/Colab Notebooks/source_test_word.txt'\n","file_target_2 = 'drive/My Drive/Colab Notebooks/target_test_emotion.txt'\n","\n","word2id, id2word = dpros.index_opensub()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q86_Ildfa3DE"},"source":["\n","def process_data(file_source, file_target, w2idx):\n","\n","    lines_source = dpros.read_lines(filename=file_source)\n","    lines_target = dpros.read_lines(filename=file_target)\n","\n","    # make every character lower case\n","    lines_source = [ line.lower() for line in lines_source ]\n","    lines_target = [ line.lower() for line in lines_target ]\n","\n","    lines_source = [ dpros.filter_line_opensub(line) for line in lines_source ]\n","    lines_target = [ dpros.filter_line_opensub(line) for line in lines_target ]\n","\n","    q_sentence, a_sentence = dpros.filter_data_opensub(lines_source, lines_target)\n","\n","    alines = []\n","    tag = []\n","    \n","    # separate the emotion from the sentence\n","    for sentence in a_sentence:\n","        alines.append(sentence[:-1])\n","        tag.append(sentence[-1])\n","\n","    # converts the lists of lines into lists of lists of words\n","    qtokenized = [ wordlist.split(' ') for wordlist in q_sentence ]\n","    atokenized = [ wordlist.split(' ') for wordlist in alines ]\n","\n","    idx_q, q_length = dpros.zero_pad_source(qtokenized, w2idx)\n","    idx_a = dpros.zero_pad_target(atokenized, w2idx)\n","\n","    return idx_q, q_length, idx_a, tag\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ou858DE3cAjo"},"source":["\n","source_train_full, length_train_full, target_train_full, tag_train_full = process_data(file_source_1, file_target_1, word2id)\n","source_test, length_test, target_test, tag_test = process_data(file_source_2, file_target_2, word2id)\n","\n","source_train_full_emotion, length_trains = dpros.emotion_pad(source_train_full, length_train_full, tag_train_full, word2id)\n","source_test_emotion, length_tests = dpros.emotion_pad(source_test, length_test, tag_test, word2id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OfNManiCXx8"},"source":["\n","## divide the sentences in train and validation. Then, create the dataset objects for the three sets:\n","\n","valid_index = int(len(source_train_full) - int(0.95 * len(source_train_full)))\n","train_index = int(0.95 * len(source_train_full))\n","\n","source_train = [source_train_full_emotion[i] for i in range(0, train_index)]\n","target_train = [target_train_full[i] for i in range(0, train_index)]\n","length_train = [length_trains[i] for i in range(0, train_index)]\n","\n","source_valid = [source_train_full_emotion[i] for i in range(train_index + 1, train_index + valid_index)]\n","target_valid = [target_train_full[i] for i in range(train_index + 1, train_index + valid_index)]\n","length_valid = [length_trains[i] for i in range(train_index + 1, train_index + valid_index)]\n","\n","training_set = utils.TensorDataset(torch.LongTensor(source_train), torch.LongTensor(length_train), torch.LongTensor(target_train))\n","valid_set = utils.TensorDataset(torch.LongTensor(source_valid), torch.LongTensor(length_valid), torch.LongTensor(target_valid))\n","test_set = utils.TensorDataset(torch.LongTensor(source_test_emotion), torch.LongTensor(length_tests), torch.LongTensor(target_test))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_MepnmUHeOYf"},"source":["## Model 1: Enc-bef"]},{"cell_type":"code","metadata":{"id":"ZAPh-7wN7kKz"},"source":["\n","class Final_model_light(pl.LightningModule):\n","    \n","    def __init__(self, emb_dim, enc_hid_dim, hid_dim, vocab_size, num_directions, pad_len, encoder, decoder, dropout):\n","        \n","        super(Final_model_light, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n","\n","        self.encoder2decoder = nn.Linear(enc_hid_dim * num_directions, hid_dim)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hid_dim, vocab_size)\n","\n","    ## the forward method that makes the operations of the emotion embedding model and outputs the predictions\n","        \n","    def forward(self, input_sentence, input_length, target):\n","    \n","        input_sentence = self.embeddings(input_sentence)\n","        target = self.embeddings(target)\n","\n","        src, (src_hidden, src_cell) = self.encoder(input_sentence, input_length)\n","\n","        decoder_init_state = self.encoder2decoder(src_hidden)\n","\n","        decoder_init_state = self.tanh(decoder_init_state)\n","\n","        context = src.transpose(0, 1)\n","\n","        trg, (hidden_trg, cell_trg) = self.decoder(target, (decoder_init_state, src_cell), context)\n","\n","        logits = self.out(trg)\n","\n","        return logits\n","    \n","    ## updates the embedding weights with the pre-trained weights:\n","\n","    def load_word_embedding(self, id2word):\n","\n","        embeddings_index = {}\n","        f = open('drive/My Drive/Colab Notebooks/cc.en.300.vec', 'r', encoding='utf-8')\n","        for line in tqdm(f):\n","            values = line.rstrip().rsplit(' ')\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","        f.close()\n","\n","        embedding_matrix = np.zeros((vocab_size, emb_dim))\n","        for i, word in id2word.items():\n","            if i < vocab_size:\n","                embedding_vector = embeddings_index.get(word)\n","                if embedding_vector is not None:\n","                    embedding_matrix[i] = embedding_vector\n","                else:\n","                    if word == '<pad>':\n","                        embedding_matrix[i] = np.zeros([emb_dim])\n","                    else:\n","                        embedding_matrix[i] = np.random.uniform(-1, 1, emb_dim)\n","        self.embeddings.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n","        self.embeddings.requires_grad = False\n","\n","    ## definition of the training step:\n","    \n","    def training_step(self, batch, batch_idx):\n","        \n","        src, src_length, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        # makes the forward pass\n","\n","        decoder_logit = self.forward(src, src_length, trg_1)\n","\n","        ## calculate the loss\n","\n","        loss = criterion(decoder_logit.contiguous().view(-1, vocab_size), trg_2.contiguous().view(-1))\n","\n","        return {'loss': loss}\n","\n","    ## at the end of the training step, print the average loss\n","\n","    def training_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n","        print(\"Train loss: \", avg_loss)\n","        return {'training_loss': avg_loss}\n","    \n","    def validation_step(self, batch, batch_idx):\n","        \n","        src, src_length, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        # makes the forward pass\n","\n","        decoder_logit = self.forward(src, src_length, trg_1)\n","\n","        ## calculate the loss\n","\n","        val_loss = criterion(decoder_logit.contiguous().view(-1, vocab_size), trg_2.contiguous().view(-1))\n","\n","        return {'val_loss': val_loss}\n","\n","    ## at the end of the validation step, print the average loss\n","    \n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        print(\"Avg loss: \", avg_loss)\n","        return {'val_loss': avg_loss}\n","    \n","    def test_step(self, batch, batch_idx):\n","        \n","        src, src_length, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        # makes the forward pass\n","\n","        decoder_logit = self.forward(src, src_length, trg_1)\n","\n","        # calculate the loss\n","\n","        test_loss = criterion(decoder_logit.contiguous().view(-1, vocab_size), trg_2.contiguous().view(-1))\n","\n","        return {'test_loss': test_loss}\n","\n","    ## at the end of the test step, print the average loss\n","    \n","    def test_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        return {'test_loss': avg_loss}\n","    \n","    def configure_optimizers(self):\n","        return optim.Adam(self.parameters(), lr=0.0001)\n","    \n","    def train_dataloader(self):\n","        \n","        train_loader = DataLoader(training_set, batch_size=dpros.batch_size, shuffle=True, num_workers=3)\n","        \n","        return train_loader\n","    \n","    def val_dataloader(self):\n","        \n","        valid_loader = DataLoader(valid_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return valid_loader\n","    \n","    def test_dataloader(self):\n","        \n","        test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return test_loader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXuqvnHQXhCP"},"source":["### Train model"]},{"cell_type":"code","metadata":{"id":"QqGG3nKPSd70"},"source":["\n","input_dim = len(word2id)\n","pad_len = 30\n","emb_dim = 300\n","dec_hid_dim = 600\n","enc_hid_dim = 300\n","hid_dim = 600\n","dropout = 0.2\n","vocab_size = len(word2id)\n","num_directions = 2\n","target_pad = word2id['<pad>']\n","\n","encoder = EM.Encoder(input_dim, emb_dim, enc_hid_dim, dropout)\n","decoder = EM.Decoder(input_dim, emb_dim, dec_hid_dim, dropout)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJwJahpuoogv"},"source":["\n","criterion = nn.CrossEntropyLoss().cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WpwJv_YiV0z-"},"source":["\n","light_model = Final_model_light(emb_dim, enc_hid_dim, hid_dim, vocab_size, num_directions, pad_len, encoder, decoder, dropout)\n","light_model.load_word_embedding(id2word)\n","\n","checkpoint_callback = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Model_opensub_checkpoint/', save_top_k=1, verbose=True, monitor='val_loss',mode='min')\n","\n","trainer = pl.Trainer(gpus=1, max_epochs=2, log_save_interval=100000, weights_summary=None, progress_bar_refresh_rate=0, log_gpu_memory=None, default_root_dir='drive/My Drive/Colab Notebooks/Model_opensub_checkpoint/', checkpoint_callback=checkpoint_callback)\n","\n","## trainer = pl.Trainer(resume_from_checkpoint='drive/My Drive/Colab Notebooks/Model_opensub_checkpoint/epoch=10.ckpt', gpus=1, max_epochs=11, checkpoint_callback=checkpoint_callback, log_save_interval=100000, weights_summary=None, progress_bar_refresh_rate=0, log_gpu_memory=None, default_root_dir='drive/My Drive/Colab Notebooks/Model_opensub_checkpoint/')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IflpXhTU7kLA"},"source":["\n","trainer.fit(light_model)\n","trainer.test(light_model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa_4YUSl7kLT"},"source":["## Evaluation metrics"]},{"cell_type":"code","metadata":{"id":"0r06yC0P7kLW"},"source":["\n","source_test_emotion, length_tests = dpros.emotion_pad(source_test[:len(source_test)//10], length_test[:len(length_test)//10], tag_test[:len(tag_test)//10], word2id)\n","test_set = utils.TensorDataset(torch.LongTensor(source_test_emotion), torch.LongTensor(length_tests), torch.LongTensor(target_test[:len(target_test)//10]))\n","\n","test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","\n","answers, predict = test_func.create_answers_preds(light_model, test_loader, word2id, id2word)\n","\n","print(\"Created successfully!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgbzdawFi7jN"},"source":["\n","for i in range(0, len(answers)):\n","\n","    answers[i] = answers[i].split('<pad>')[0]\n","    predict[i] = predict[i].split('<pad>')[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrHPby307kLa"},"source":["### BLEU"]},{"cell_type":"code","metadata":{"id":"nxsjFaw6i-IT"},"source":["\n","test_func.bleu_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Z_1HMp67kLd"},"source":["### ROUGE"]},{"cell_type":"code","metadata":{"id":"UG3OTH__jA_H"},"source":["\n","test_func.rouge_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BEj2sxr7kLn"},"source":["### Embedding average metric"]},{"cell_type":"code","metadata":{"id":"zeIeHncCjD20"},"source":["\n","test_func.word_embedding_scores(answers, predict, light_model, word2id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLBHFXf21U6j"},"source":["\n","## this is for creating the predictions for each emotion and store them in separate files\n","\n","for tag in range(5):\n","    source_test_emotion, length_tests = dpros.emotion_pad(source_test[:len(source_test)//10], length_test[:len(length_test)//10], tag, word2id)\n","    test_set = utils.TensorDataset(torch.LongTensor(source_test_emotion), torch.LongTensor(length_tests), torch.LongTensor(target_test[:len(target_test)//10]))\n","    test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","    sources, predict = test_func.create_sources_preds(light_model, test_loader, word2id, id2word)\n","\n","    print(\"Emotion \" + str(tag))\n","\n","    df = pd.DataFrame({'Sources': [''.join(source_test) for source_test in sources], 'Predictions': [''.join(predicted_test) for predicted_test in predict]})\n","    df.to_csv('Emotion' + str(tag) + '.csv', encoding='utf-8', index=False)\n"],"execution_count":null,"outputs":[]}]}