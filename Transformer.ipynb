{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CX7Zm-sbAa1L"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wkvu-n45PmLD"},"source":["!pip install py-rouge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xzO-jLRBTal","executionInfo":{"status":"ok","timestamp":1601404673340,"user_tz":-120,"elapsed":41216,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"e694b785-f1ac-4382-8ca1-46673554fd9c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a7mNS-6VX2dU"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import math\n","import Data_processing as dpros\n","import transformer_model as TM\n","import test_functions as test_func\n","import statistics\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import pytorch_lightning as pl\n","import nltk\n","import numpy as np\n","import nltk\n","from tqdm import tqdm\n","import pandas as pd\n","import torch.utils.data as utils\n","from torch.utils.data import Dataset, DataLoader\n","import nltk.translate.bleu_score as bleu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Hx3bc0LX2da"},"source":["# Transformer"]},{"cell_type":"code","metadata":{"id":"jEYeNVHL7mpF"},"source":["\n","class Transformer_light(pl.LightningModule):\n","\n","    def __init__(self, input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers):\n","\n","        super(Transformer_light, self).__init__()\n","\n","        self.n_warmup_steps = 16000\n","        self.init_lr = 0.5\n","\n","        self.encoder = TM.Trans_encoder(input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers)\n","        self.decoder = TM.Trans_decoder(input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers)\n","\n","        self.embedding = nn.Embedding(input_dim, dim_model)\n","\n","        self.linear_out = nn.Linear(dim_model, input_dim)\n","    \n","    ## the forward method that makes the operations of the transformer and outputs the predictions\n","\n","    def forward(self, input, target, input_mask, target_mask):\n","\n","        input = self.embedding(input)\n","        target = self.embedding(target)\n","\n","        enc_sentence = self.encoder(input, input_mask)\n","\n","        dec_sentence = self.decoder(target, enc_sentence, input_mask, target_mask)\n","\n","        result = self.linear_out(dec_sentence)\n","\n","        return result\n","\n","    ## definition of the training step:\n","\n","    def training_step(self, batch, batch_idx):\n","        \n","        src, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        ## creates the masks and makes the forward pass\n","\n","        input_mask, target_mask = dpros.create_masks(src, trg_1, word2id)\n","        decoder_logit = self.forward(src, trg_1, input_mask, target_mask)\n","\n","        ## calculate the loss\n","\n","        loss = criterion(decoder_logit.contiguous().view(-1, input_dim), trg_2.contiguous().view(-1))\n","\n","        return {'loss': loss}\n","\n","    ## definition of the validation step:\n","    \n","    def validation_step(self, batch, batch_idx):\n","        \n","        src, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        ## creates the masks and makes the forward pass\n","\n","        input_mask, target_mask = dpros.create_masks(src, trg_1, word2id)\n","        decoder_logit = self.forward(src, trg_1, input_mask, target_mask)\n","\n","        ## calculate the loss\n","        \n","        val_loss = criterion(decoder_logit.contiguous().view(-1, input_dim), trg_2.contiguous().view(-1))\n","\n","        return {'val_loss': val_loss}\n","\n","    ## at the end of the validation step, print the average loss\n","    \n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        print(\"Val loss: \", avg_loss)\n","        return {'val_loss': avg_loss}\n","    \n","    ## definition of the test step:\n","    \n","    def test_step(self, batch, batch_idx):\n","        \n","        src, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        ## creates the masks and makes the forward pass\n","\n","        input_mask, target_mask = dpros.create_masks(src, trg_1, word2id)\n","        decoder_logit = self.forward(src, trg_1, input_mask, target_mask)\n","\n","        ## calculate the loss\n","        \n","        test_loss = criterion(decoder_logit.contiguous().view(-1, input_dim), trg_2.contiguous().view(-1))\n","\n","        return {'test_loss': test_loss}\n","    \n","    ## at the end of the test step, print the average loss\n","    \n","    def test_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        return {'test_loss': avg_loss}\n","    \n","    def configure_optimizers(self):\n","        return optim.Adam(self.parameters(), betas=(0.9, 0.98), eps=1e-09)\n","\n","    ## defines the optimizer step with the warmup learning rate. After the warmup steps, the learning rate is 0.0001\n","    \n","    def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure, using_native_amp, on_tpu=False, using_lbfgs=False):\n","\n","        if (self.trainer.global_step < self.n_warmup_steps) and (self.trainer.global_step > 0):\n","          lr_scale = (dim_model ** -0.5) * min(self.trainer.global_step ** (-0.5), self.trainer.global_step * (self.n_warmup_steps ** (-1.5)))\n","          for pg in optimizer.param_groups:\n","              pg['lr'] = lr_scale * self.init_lr\n","        else:\n","          for pg in optimizer.param_groups:\n","            pg['lr'] = 0.0001\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    \n","    def train_dataloader(self):\n","        \n","        train_loader = DataLoader(training_set, batch_size=dpros.batch_size, shuffle=True, num_workers=3)\n","        \n","        return train_loader\n","    \n","    def val_dataloader(self):\n","        \n","        valid_loader = DataLoader(valid_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return valid_loader\n","    \n","    def test_dataloader(self):\n","        \n","        test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return test_loader\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfCucFJgX2d8"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"U5kjgeUKzXUv"},"source":["\n","FILENAME = 'chat_emotion.txt'\n","\n","lines = dpros.read_lines(filename=FILENAME)\n","\n","# make every character lower case\n","lines = [ line.lower() for line in lines ]\n","\n","lines = [ dpros.filter_line(line, dpros.EN_WHITELIST) for line in lines ]\n","\n","qlines, asentence = dpros.filter_data(lines)\n","\n","alines = []\n","tag = []\n","\n","# separate the emotion from the sentence\n","for sentence in asentence:\n","    alines.append(sentence[:-2])\n","    tag.append(sentence[-1])\n","\n","qtokenized = [ wordlist.split(' ') for wordlist in qlines ]\n","atokenized = [ wordlist.split(' ') for wordlist in alines ]\n","\n","idx2w, w2idx = dpros.index_(qtokenized + atokenized, vocab_size=dpros.VOCAB_SIZE)\n","\n","idx_q, idx_a = dpros.zero_pad_trans(qtokenized, atokenized, w2idx)\n","\n","metadata = {'w2idx': w2idx, 'idx2w': idx2w, 'limit': dpros.limit}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Tz2J8fMderK"},"source":["\n","word2id = metadata['w2idx']\n","id2word = metadata['idx2w']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Itdz3kcaX2ew"},"source":["## Train model"]},{"cell_type":"code","metadata":{"id":"Z_hEhyNCRWE3"},"source":["\n","## divide the sentences in train, test and validation. Then, add the emotion tokens to the source sentences and finally create the dataset objects for the three sets:\n","\n","test_index = int(0.05 * len(idx_q))\n","train_index = int(len(idx_q) - test_index)\n","valid_index = int(train_index - int(0.95 * train_index))\n","train_index = int(0.95 * train_index)\n","\n","X_train = [idx_q[i] for i in range(0, train_index)]\n","y_train = [idx_a[i] for i in range(0, train_index)]\n","tag_train = [tag[i] for i in range(0, train_index)]\n","\n","X_valid = [idx_q[i] for i in range(train_index + 1, train_index + valid_index)]\n","y_valid = [idx_a[i] for i in range(train_index + 1, train_index + valid_index)]\n","tag_valid = [tag[i] for i in range(train_index + 1, train_index + valid_index)]\n","\n","X_test = [idx_q[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","y_test = [idx_a[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","tag_test = [tag[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","\n","\n","X_train_emotion = dpros.emotion_pad_trans(X_train, tag_train, word2id)\n","X_valid_emotion = dpros.emotion_pad_trans(X_valid, tag_valid, word2id)\n","X_test_emotion = dpros.emotion_pad_trans(X_test, tag_test, word2id)\n","\n","training_set = utils.TensorDataset(torch.LongTensor(X_train_emotion), torch.LongTensor(y_train))\n","valid_set = utils.TensorDataset(torch.LongTensor(X_valid_emotion), torch.LongTensor(y_valid))\n","test_set = utils.TensorDataset(torch.LongTensor(X_test_emotion), torch.LongTensor(y_test))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i3TynQsoX2e0"},"source":["\n","input_dim = len(word2id)\n","seq_len = 30\n","dim_model = 512\n","num_heads = 8\n","dim_k = dim_model / num_heads\n","dim_v = dim_model / num_heads\n","dim_ff = 2048\n","h_dim_v = num_heads * dim_v\n","num_layers = 6\n","target_pad = word2id['<pad>']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sx_xzA7XLnaf"},"source":["criterion = nn.CrossEntropyLoss().cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY5Bjeol-y8M"},"source":["\n","light_model = Transformer_light(input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers)\n","\n","## we initialize the parameters of the model\n","\n","for p in light_model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","checkpoint_callback = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Trans_checkpoint/', save_top_k=1, verbose=True, monitor='val_loss',mode='min')\n","\n","trainer = pl.Trainer(gpus=1, max_epochs=10, log_save_interval=100000, progress_bar_refresh_rate=0, weights_summary=None, log_gpu_memory=None, default_root_dir='drive/My Drive/Colab Notebooks/Trans_checkpoint/', checkpoint_callback=checkpoint_callback)\n","\n","## trainer = pl.Trainer(resume_from_checkpoint='drive/My Drive/Colab Notebooks/Trans_checkpoint/epoch=12.ckpt', gpus=1, max_epochs=13, progress_bar_refresh_rate=0, log_save_interval=100000, weights_summary=None, log_gpu_memory=None, default_root_dir='drive/My Drive/Colab Notebooks/Trans_checkpoint/', checkpoint_callback=checkpoint_callback)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUbhSCN-7msI"},"source":["\n","trainer.fit(light_model)\n","trainer.test(light_model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTzLBUHB7msS"},"source":["## Evaluation metrics"]},{"cell_type":"code","metadata":{"id":"TtS9po3P7msT","executionInfo":{"status":"ok","timestamp":1601411724883,"user_tz":-120,"elapsed":309,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"ccfcf722-71d2-4ca1-d9cb-f742253827f2","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","\n","answers, predict = test_func.create_answers_preds_trans(light_model, test_loader, word2id, id2word)\n","\n","print(\"Created successfully!\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Created successfully!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t3tQzCl8QFwN"},"source":["\n","for i in range(0, len(answers)):\n","\n","    answers[i] = answers[i].split('<pad>')[0]\n","    predict[i] = predict[i].split('<pad>')[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T06zeCQ47msV"},"source":["### BLEU"]},{"cell_type":"code","metadata":{"id":"j2LjrbzUQMz0"},"source":["\n","test_func.bleu_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5fNdt7V7msY"},"source":["### ROUGE"]},{"cell_type":"code","metadata":{"id":"IbXxpgX3QPTS"},"source":["\n","test_func.rouge_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vEWLZAbT7msg"},"source":["### Embedding average metric"]},{"cell_type":"code","metadata":{"id":"2TiVN5jEQSDr"},"source":["\n","test_func.word_embedding_scores_trans(answers, predict, light_model, word2id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-cSOLZK6QW-P"},"source":["\n","## this is for creating the predictions for each emotion and store them in separate files\n","\n","for tag in range(9):\n","    X_test_emotion = dpros.emotion_pad_trans(X_test, tag, word2id)\n","    test_set = utils.TensorDataset(torch.LongTensor(X_test_emotion), torch.LongTensor(y_test))\n","    test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","    sources, predict = test_func.create_sources_preds_trans(light_model, test_loader, word2id, id2word)\n","\n","    print(\"Emotion \" + str(tag))\n","\n","    df = pd.DataFrame({'Sources': [''.join(source_test) for source_test in sources], 'Predictions': [''.join(predicted_test) for predicted_test in predict]})\n","    df.to_csv('Transformer' + str(tag) + '.csv', encoding='utf-8', index=False)\n"],"execution_count":null,"outputs":[]}]}