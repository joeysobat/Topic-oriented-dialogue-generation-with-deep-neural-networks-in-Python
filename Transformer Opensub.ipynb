{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer Opensub.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CX7Zm-sbAa1L"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wkvu-n45PmLD"},"source":["!pip install py-rouge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbItwxhMklnp"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7mNS-6VX2dU"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import math\n","import Data_processing as dpros\n","import transformer_model as TM\n","import test_functions as test_func\n","import statistics\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import pytorch_lightning as pl\n","import nltk\n","import numpy as np\n","import nltk\n","from tqdm import tqdm\n","import torch.utils.data as utils\n","from torch.utils.data import Dataset, DataLoader\n","import nltk.translate.bleu_score as bleu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Hx3bc0LX2da"},"source":["# Transformer"]},{"cell_type":"code","metadata":{"id":"jEYeNVHL7mpF"},"source":["\n","class Transformer_light(pl.LightningModule):\n","\n","    def __init__(self, input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers):\n","\n","        super(Transformer_light, self).__init__()\n","\n","        self.n_warmup_steps = 16000\n","        self.init_lr = 0.5\n","\n","        self.encoder = TM.Trans_encoder(input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers)\n","        self.decoder = TM.Trans_decoder(input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers)\n","\n","        self.embedding = nn.Embedding(input_dim, dim_model)\n","\n","        self.linear_out = nn.Linear(dim_model, input_dim)\n","    \n","    ## the forward method that makes the operations of the transformer and outputs the predictions\n","\n","    def forward(self, input, target, input_mask, target_mask):\n","\n","        input = self.embedding(input)\n","        target = self.embedding(target)\n","\n","        enc_sentence = self.encoder(input, input_mask)\n","\n","        dec_sentence = self.decoder(target, enc_sentence, input_mask, target_mask)\n","\n","        result = self.linear_out(dec_sentence)\n","\n","        return result\n","\n","    ## definition of the training step:\n","\n","    def training_step(self, batch, batch_idx):\n","        \n","        src, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        ## creates the masks and makes the forward pass\n","\n","        input_mask, target_mask = dpros.create_masks(src, trg_1, word2id)\n","        decoder_logit = self.forward(src, trg_1, input_mask, target_mask)\n","\n","        ## calculate the loss\n","\n","        loss = criterion(decoder_logit.contiguous().view(-1, input_dim), trg_2.contiguous().view(-1))\n","\n","        return {'loss': loss}\n","\n","    ## definition of the validation step:\n","    \n","    def validation_step(self, batch, batch_idx):\n","        \n","        src, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        ## creates the masks and makes the forward pass\n","\n","        input_mask, target_mask = dpros.create_masks(src, trg_1, word2id)\n","        decoder_logit = self.forward(src, trg_1, input_mask, target_mask)\n","\n","        ## calculate the loss\n","        \n","        val_loss = criterion(decoder_logit.contiguous().view(-1, input_dim), trg_2.contiguous().view(-1))\n","\n","        return {'val_loss': val_loss}\n","\n","    ## at the end of the validation step, print the average loss\n","    \n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        print(\"Val loss: \", avg_loss)\n","        return {'val_loss': avg_loss}\n","    \n","    ## definition of the test step:\n","    \n","    def test_step(self, batch, batch_idx):\n","        \n","        src, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        ## creates the masks and makes the forward pass\n","\n","        input_mask, target_mask = dpros.create_masks(src, trg_1, word2id)\n","        decoder_logit = self.forward(src, trg_1, input_mask, target_mask)\n","\n","        ## calculate the loss\n","        \n","        test_loss = criterion(decoder_logit.contiguous().view(-1, input_dim), trg_2.contiguous().view(-1))\n","\n","        return {'test_loss': test_loss}\n","    \n","    ## at the end of the test step, print the average loss\n","    \n","    def test_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        return {'test_loss': avg_loss}\n","    \n","    def configure_optimizers(self):\n","        return optim.Adam(self.parameters(), betas=(0.9, 0.98), eps=1e-09)\n","\n","    ## defines the optimizer step with the warmup learning rate. After the warmup steps, the learning rate is 0.0001\n","    \n","    def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure, using_native_amp, on_tpu=False, using_lbfgs=False):\n","\n","        if (self.trainer.global_step < self.n_warmup_steps) and (self.trainer.global_step > 0):\n","          lr_scale = (dim_model ** -0.5) * min(self.trainer.global_step ** (-0.5), self.trainer.global_step * (self.n_warmup_steps ** (-1.5)))\n","          for pg in optimizer.param_groups:\n","              pg['lr'] = lr_scale * self.init_lr\n","        else:\n","          for pg in optimizer.param_groups:\n","            pg['lr'] = 0.0001\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    \n","    def train_dataloader(self):\n","        \n","        train_loader = DataLoader(training_set, batch_size=dpros.batch_size, shuffle=True, num_workers=3)\n","        \n","        return train_loader\n","    \n","    def val_dataloader(self):\n","        \n","        valid_loader = DataLoader(valid_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return valid_loader\n","    \n","    def test_dataloader(self):\n","        \n","        test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return test_loader\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vfCucFJgX2d8"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"RCyeeh9eCXx1"},"source":["\n","def process_data(file_source, file_target, w2idx):\n","\n","    lines_source = dpros.read_lines(filename=file_source)\n","    lines_target = dpros.read_lines(filename=file_target)\n","\n","    # make every character lower case\n","    lines_source = [ line.lower() for line in lines_source ]\n","    lines_target = [ line.lower() for line in lines_target ]\n","\n","    lines_source = [ dpros.filter_line_opensub(line) for line in lines_source ]\n","    lines_target = [ dpros.filter_line_opensub(line) for line in lines_target ]\n","\n","    q_sentence, a_sentence = dpros.filter_data_opensub(lines_source, lines_target)\n","\n","    alines = []\n","    tag = []\n","    \n","    # separate the emotion from the sentence\n","    for sentence in a_sentence:\n","        alines.append(sentence[:-1])\n","        tag.append(sentence[-1])\n","\n","    qtokenized = [ wordlist.split(' ') for wordlist in q_sentence ]\n","    atokenized = [ wordlist.split(' ') for wordlist in alines ]\n","\n","    idx_q, idx_a = dpros.zero_pad_trans(qtokenized, atokenized, w2idx)\n","\n","    return idx_q, idx_a, tag\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YCXj0dY9ovT"},"source":["\n","word2id, id2word = dpros.index_opensub()\n","\n","source_train_full, target_train_full, tag_train_full = process_data('drive/My Drive/Colab Notebooks/source_train_word.txt', 'drive/My Drive/Colab Notebooks/target_train_emotion.txt', word2id)\n","source_test, target_test, tag_test = process_data('drive/My Drive/Colab Notebooks/source_test_word.txt', 'drive/My Drive/Colab Notebooks/target_test_emotion.txt', word2id)\n","\n","source_train_full_emotion = dpros.emotion_pad_trans(source_train_full, tag_train_full, word2id)\n","source_test_emotion = dpros.emotion_pad_trans(source_test, tag_test, word2id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OfNManiCXx8"},"source":["\n","## divide the sentences in train and validation. Then, create the dataset objects for the three sets:\n","\n","valid_index = int(len(source_train_full_emotion) - int(0.95 * len(source_train_full_emotion)))\n","train_index = int(0.95 * len(source_train_full_emotion))\n","\n","source_train = [source_train_full_emotion[i] for i in range(0, train_index)]\n","target_train = [target_train_full[i] for i in range(0, train_index)]\n","\n","source_valid = [source_train_full_emotion[i] for i in range(train_index + 1, train_index + valid_index)]\n","target_valid = [target_train_full[i] for i in range(train_index + 1, train_index + valid_index)]\n","\n","training_set = utils.TensorDataset(torch.LongTensor(source_train), torch.LongTensor(target_train))\n","valid_set = utils.TensorDataset(torch.LongTensor(source_valid), torch.LongTensor(target_valid))\n","test_set = utils.TensorDataset(torch.LongTensor(source_test_emotion), torch.LongTensor(target_test))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Itdz3kcaX2ew"},"source":["## Train model"]},{"cell_type":"code","metadata":{"id":"i3TynQsoX2e0"},"source":["\n","input_dim = len(word2id)\n","seq_len = 30\n","dim_model = 512\n","num_heads = 8\n","dim_k = dim_model / num_heads\n","dim_v = dim_model / num_heads\n","dim_ff = 2048\n","h_dim_v = num_heads * dim_v\n","num_layers = 6\n","target_pad = word2id['<pad>']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUX3JQWEX2e6"},"source":["\n","criterion = nn.CrossEntropyLoss().cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY5Bjeol-y8M"},"source":["\n","light_model = Transformer_light(input_dim, dim_model, h_dim_v, dim_k, dim_v, dim_ff, seq_len, num_heads, num_layers)\n","\n","## we initialize the parameters of the model\n","\n","for p in light_model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","checkpoint_callback = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Trans_opensub_checkpoint/', save_top_k=1, verbose=True, monitor='val_loss',mode='min')\n","\n","trainer = pl.Trainer(gpus=1, max_epochs=2, log_save_interval=100000, progress_bar_refresh_rate=0, weights_summary=None, log_gpu_memory=None, default_root_dir='drive/My Drive/Colab Notebooks/Trans_opensub_checkpoint/', checkpoint_callback=checkpoint_callback)\n","## trainer = pl.Trainer(resume_from_checkpoint='drive/My Drive/Colab Notebooks/Trans_opensub_checkpoint/epoch=11.ckpt', gpus=1, max_epochs=12, log_save_interval=100000, progress_bar_refresh_rate=0, weights_summary=None, log_gpu_memory=None, default_root_dir='drive/My Drive/Colab Notebooks/Trans_opensub_checkpoint/', checkpoint_callback=checkpoint_callback)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUbhSCN-7msI"},"source":["\n","trainer.fit(light_model)\n","trainer.test(light_model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTzLBUHB7msS"},"source":["## Evaluation metrics"]},{"cell_type":"code","metadata":{"id":"TtS9po3P7msT"},"source":["\n","source_test_emotion = dpros.emotion_pad_trans(source_test[:len(source_test)//10], tag_test[:len(tag_test)//10], word2id)\n","test_set = utils.TensorDataset(torch.LongTensor(source_test_emotion), torch.LongTensor(target_test[:len(target_test)//10]))\n","\n","test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","\n","answers, predict = test_func.create_answers_preds_trans(light_model, test_loader, word2id, id2word)\n","\n","print(\"Created successfully!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CAAWVRAPFzZ"},"source":["\n","for i in range(0, len(answers)):\n","\n","    answers[i] = answers[i].split('<pad>')[0]\n","    predict[i] = predict[i].split('<pad>')[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T06zeCQ47msV"},"source":["### BLEU"]},{"cell_type":"code","metadata":{"id":"YfjHKi_tPMZo"},"source":["\n","test_func.bleu_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5fNdt7V7msY"},"source":["### ROUGE"]},{"cell_type":"code","metadata":{"id":"q1WiP8GTPP1Q"},"source":["\n","test_func.rouge_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vEWLZAbT7msg"},"source":["### Embedding average metric"]},{"cell_type":"code","metadata":{"id":"ylFZ52F4PSDO"},"source":["\n","test_func.word_embedding_scores_trans(answers, predict, light_model, word2id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSx2FQhUbnwN"},"source":["\n","## this is for creating the predictions for each emotion and store them in separate files\n","\n","for tag in range(4):\n","    source_test_emotion = dpros.emotion_pad_trans(source_test[:len(source_test)//10], tag, word2id)\n","    test_set = utils.TensorDataset(torch.LongTensor(source_test_emotion), torch.LongTensor(target_test[:len(target_test)//10]))\n","    test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","    sources, predict = test_func.create_sources_preds_trans(light_model, test_loader, word2id, id2word)\n","\n","    print(\"Emotion \" + str(tag))\n","\n","    df = pd.DataFrame({'Sources': [''.join(source_test) for source_test in sources], 'Predictions': [''.join(predicted_test) for predicted_test in predict]})\n","    df.to_csv('Transformer' + str(tag) + '.csv', encoding='utf-8', index=False)\n"],"execution_count":null,"outputs":[]}]}