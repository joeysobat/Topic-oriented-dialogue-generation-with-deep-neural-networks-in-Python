{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Emotion Classifier.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"cells":[{"cell_type":"code","metadata":{"id":"_gzEeTQqOjzF"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"foLOLJ1hI2N5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40GiOpWvFEyV"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"OPLXraeBE4xY"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","import itertools\n","import numpy as np\n","import torch.utils.data as utils\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from torch.autograd import Variable\n","import Data_processing_classifier as dpros\n","import Data_processing as dpros2\n","import Emotion_classifier as EM\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","import scipy\n","from sklearn.metrics import confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKKneU6oxEiX"},"source":["## Data preprocessing"]},{"cell_type":"code","metadata":{"id":"FP__88Y0G3JE","executionInfo":{"status":"ok","timestamp":1599812265136,"user_tz":-120,"elapsed":38291,"user":{"displayName":"Ander M","photoUrl":"","userId":"13363892476313937594"}},"outputId":"e49bdc2d-f401-4aad-e07a-c50daf24ae17","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","stopwords_dict = Counter(stop_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gThYtYe3UlgV"},"source":["\n","## the list of contractions to substitute\n","\n","contractions = { \n","\"ain t\": \"am not\",\n","\"aren t\": \"are not\",\n","\"can t\": \"cannot\",\n","\"can t ve\": \"cannot have\",\n","\" cause\": \"because\",\n","\"could ve\": \"could have\",\n","\"couldn t\": \"could not\",\n","\"couldn t ve\": \"could not have\",\n","\"didn t\": \"did not\",\n","\"doesn t\": \"does not\",\n","\"don t\": \"do not\",\n","\"hadn t\": \"had not\",\n","\"hadn t ve\": \"had not have\",\n","\"hasn t\": \"has not\",\n","\"haven t\": \"have not\",\n","\"he d\": \"he would\",\n","\"he d ve\": \"he would have\",\n","\"he ll\": \"he will\",\n","\"he ll ve\": \"he will have\",\n","\"he s\": \"he is\",\n","\"how d\": \"how did\",\n","\"how d y\": \"how do you\",\n","\"how ll\": \"how will\",\n","\"how s\": \"how is\",\n","\"I d\": \"I would\",\n","\"I d ve\": \"I would have\",\n","\"I ll\": \"I will\",\n","\"I ll ve\": \"I will have\",\n","\"I m\": \"I am\",\n","\"I ve\": \"I have\",\n","\"isn t\": \"is not\",\n","\"it d\": \"it would\",\n","\"it d ve\": \"it would have\",\n","\"it ll\": \"it will\",\n","\"it ll ve\": \"it will have\",\n","\"it s\": \"it is\",\n","\"let s\": \"let us\",\n","\"ma am\": \"madam\",\n","\"mayn t\": \"may not\",\n","\"might ve\": \"might have\",\n","\"mightn t\": \"might not\",\n","\"mightn t ve\": \"might not have\",\n","\"must ve\": \"must have\",\n","\"mustn t\": \"must not\",\n","\"mustn t ve\": \"must not have\",\n","\"needn t\": \"need not\",\n","\"needn t ve\": \"need not have\",\n","\"o clock\": \"of the clock\",\n","\"oughtn t\": \"ought not\",\n","\"oughtn t ve\": \"ought not have\",\n","\"shan t\": \"shall not\",\n","\"sha n t\": \"shall not\",\n","\"shan t ve\": \"shall not have\",\n","\"she d\": \"she would\",\n","\"she d ve\": \"she would have\",\n","\"she ll\": \"she will\",\n","\"she ll ve\": \"she will have\",\n","\"she s\": \"she is\",\n","\"should ve\": \"should have\",\n","\"shouldn t\": \"should not\",\n","\"shouldn t ve\": \"should not have\",\n","\"so ve\": \"so have\",\n","\"so s\": \"so as\",\n","\"that d\": \"that would\",\n","\"that d ve\": \"that would have\",\n","\"that s\": \"that is\",\n","\"there d\": \"there would\",\n","\"there d ve\": \"there would have\",\n","\"there s\": \"there is\",\n","\"they d\": \"they would\",\n","\"they d ve\": \"they would have\",\n","\"they ll\": \"they will\",\n","\"they ll ve\": \"they will have\",\n","\"they re\": \"they are\",\n","\"they ve\": \"they have\",\n","\"to ve\": \"to have\",\n","\"wasn t\": \"was not\",\n","\"we d\": \"we would\",\n","\"we d ve\": \"we would have\",\n","\"we ll\": \"we will\",\n","\"we ll ve\": \"we will have\",\n","\"we re\": \"we are\",\n","\"we ve\": \"we have\",\n","\"weren t\": \"were not\",\n","\"what ll\": \"what will\",\n","\"what ll ve\": \"what will have\",\n","\"what re\": \"what are\",\n","\"what s\": \"what is\",\n","\"what ve\": \"what have\",\n","\"when s\": \"when is\",\n","\"when ve\": \"when have\",\n","\"where d\": \"where did\",\n","\"where s\": \"where is\",\n","\"where ve\": \"where have\",\n","\"who ll\": \"who will\",\n","\"who ll ve\": \"who will have\",\n","\"who s\": \"who is\",\n","\"who ve\": \"who have\",\n","\"why s\": \"why is\",\n","\"why ve\": \"why have\",\n","\"will ve\": \"will have\",\n","\"won t\": \"will not\",\n","\"won t ve\": \"will not have\",\n","\"would ve\": \"would have\",\n","\"wouldn t\": \"would not\",\n","\"wouldn t ve\": \"would not have\",\n","\"y all\": \"you all\",\n","\"y all d\": \"you all would\",\n","\"y all d ve\": \"you all would have\",\n","\"y all re\": \"you all are\",\n","\"y all ve\": \"you all have\",\n","\"you d\": \"you would\",\n","\"you d ve\": \"you would have\",\n","\"you ll\": \"you will\",\n","\"you ll ve\": \"you will have\",\n","\"you re\": \"you are\",\n","\"you ve\": \"you have\"\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PyhIWnfo8HvO"},"source":["\n","FILENAME = \"CBET.csv\"\n","\n","lines = dpros.read_lines(FILENAME)\n","\n","train_lines = []\n","emotions = []\n","\n","\n","## separate the sentences and the emotions\n","for (i, line) in enumerate(lines[1:]):\n","  line_split = dpros.split_line(line)[1:]\n","  train_lines.append(line_split[0])\n","  emotion_list = list(map(float, line_split[1:]))\n","  emotion = emotion_list.index(1)\n","  emotions.append(emotion)\n","\n","lines = [dpros.filter_line(line, stopwords_dict, contractions) for line in train_lines]\n","\n","lines_filtered, list_emotion = dpros.filter_data(lines, emotions)\n","\n","lines_tokenized = [wordlist.split(' ') for wordlist in lines_filtered]\n","\n","idx2w, w2idx, freq_dist = dpros.index_(lines_tokenized, vocab_size=dpros.VOCAB_SIZE)\n","\n","list_lines = dpros.zero_pad(lines_tokenized, w2idx)\n","\n","metadata = {'w2idx': w2idx, 'idx2w': idx2w, 'limit': dpros.limit}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7bID0tzwbBw"},"source":["\n","## divide the emotions by groups, to get a balance distribution\n","\n","list_lines_0 = []\n","list_emotions_0 = []\n","list_lines_1 = []\n","list_emotions_1 = []\n","list_lines_2 = []\n","list_emotions_2 = []\n","list_lines_3 = []\n","list_emotions_3 = []\n","list_lines_4 = []\n","list_emotions_4 = []\n","list_lines_5 = []\n","list_emotions_5 = []\n","list_lines_6 = []\n","list_emotions_6 = []\n","list_lines_7 = []\n","list_emotions_7 = []\n","list_lines_8 = []\n","list_emotions_8 = []\n","\n","for i in range(0, 76859):\n","\n","    if list_emotion[i] == 0:\n","        list_lines_0.append(list_lines[i])\n","        list_emotions_0.append(list_emotion[i])\n","    elif list_emotion[i] == 1:\n","        list_lines_1.append(list_lines[i])\n","        list_emotions_1.append(list_emotion[i])\n","    elif list_emotion[i] == 2:\n","        list_lines_2.append(list_lines[i])\n","        list_emotions_2.append(list_emotion[i])\n","    elif list_emotion[i] == 3:\n","        list_lines_3.append(list_lines[i])\n","        list_emotions_3.append(list_emotion[i])\n","    elif list_emotion[i] == 4:\n","        list_lines_4.append(list_lines[i])\n","        list_emotions_4.append(list_emotion[i])\n","    elif list_emotion[i] == 5:\n","        list_lines_5.append(list_lines[i])\n","        list_emotions_5.append(list_emotion[i])\n","    elif list_emotion[i] == 6:\n","        list_lines_6.append(list_lines[i])\n","        list_emotions_6.append(list_emotion[i])\n","    elif list_emotion[i] == 7:\n","        list_lines_7.append(list_lines[i])\n","        list_emotions_7.append(list_emotion[i])\n","    elif list_emotion[i] == 8:\n","        list_lines_8.append(list_lines[i])\n","        list_emotions_8.append(list_emotion[i])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-B4rlKFg9M9"},"source":["\n","## divide the sentences and emotions in train, test and validation sets\n","\n","test_index = int(0.05 * len(list_lines_0))\n","train_index = int(len(list_lines_0) - test_index)\n","valid_index = int(train_index - int(0.95 * train_index))\n","train_index = int(0.95 * train_index)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDi-BCjjZPv5"},"source":["\n","lines_train = []\n","emotion_train = []\n","\n","lines_valid = []\n","emotion_valid = []\n","\n","lines_test = []\n","emotion_test = []\n","\n","for i in range(0, train_index):\n","\n","    lines_train.append(list_lines_0[i])\n","    lines_train.append(list_lines_1[i])\n","    lines_train.append(list_lines_2[i])\n","    lines_train.append(list_lines_3[i])\n","    lines_train.append(list_lines_4[i])\n","    lines_train.append(list_lines_5[i])\n","    lines_train.append(list_lines_6[i])\n","    lines_train.append(list_lines_7[i])\n","    lines_train.append(list_lines_8[i])\n","\n","    emotion_train.append(list_emotions_0[i])\n","    emotion_train.append(list_emotions_1[i])\n","    emotion_train.append(list_emotions_2[i])\n","    emotion_train.append(list_emotions_3[i])\n","    emotion_train.append(list_emotions_4[i])\n","    emotion_train.append(list_emotions_5[i])\n","    emotion_train.append(list_emotions_6[i])\n","    emotion_train.append(list_emotions_7[i])\n","    emotion_train.append(list_emotions_8[i])\n","\n","for j in range(train_index + 1, train_index + valid_index):\n","\n","    lines_valid.append(list_lines_0[j])\n","    lines_valid.append(list_lines_1[j])\n","    lines_valid.append(list_lines_2[j])\n","    lines_valid.append(list_lines_3[j])\n","    lines_valid.append(list_lines_4[j])\n","    lines_valid.append(list_lines_5[j])\n","    lines_valid.append(list_lines_6[j])\n","    lines_valid.append(list_lines_7[j])\n","    lines_valid.append(list_lines_8[j])\n","\n","    emotion_valid.append(list_emotions_0[j])\n","    emotion_valid.append(list_emotions_1[j])\n","    emotion_valid.append(list_emotions_2[j])\n","    emotion_valid.append(list_emotions_3[j])\n","    emotion_valid.append(list_emotions_4[j])\n","    emotion_valid.append(list_emotions_5[j])\n","    emotion_valid.append(list_emotions_6[j])\n","    emotion_valid.append(list_emotions_7[j])\n","    emotion_valid.append(list_emotions_8[j])\n","\n","for k in range(train_index + valid_index + 1, len(list_lines_0) - 1):\n","\n","    lines_test.append(list_lines_0[k])\n","    lines_test.append(list_lines_1[k])\n","    lines_test.append(list_lines_2[k])\n","    lines_test.append(list_lines_3[k])\n","    lines_test.append(list_lines_4[k])\n","    lines_test.append(list_lines_5[k])\n","    lines_test.append(list_lines_6[k])\n","    lines_test.append(list_lines_7[k])\n","    lines_test.append(list_lines_8[k])\n","\n","    emotion_test.append(list_emotions_0[k])\n","    emotion_test.append(list_emotions_1[k])\n","    emotion_test.append(list_emotions_2[k])\n","    emotion_test.append(list_emotions_3[k])\n","    emotion_test.append(list_emotions_4[k])\n","    emotion_test.append(list_emotions_5[k])\n","    emotion_test.append(list_emotions_6[k])\n","    emotion_test.append(list_emotions_7[k])\n","    emotion_test.append(list_emotions_8[k])\n","\n","train_dataset = utils.TensorDataset(torch.LongTensor(lines_train), torch.LongTensor(emotion_train))\n","valid_dataset = utils.TensorDataset(torch.LongTensor(lines_valid), torch.LongTensor(emotion_valid))\n","test_dataset = utils.TensorDataset(torch.LongTensor(lines_test), torch.LongTensor(emotion_test))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILBWbT4Axucq"},"source":["## Emotion classifier"]},{"cell_type":"code","metadata":{"id":"5xpBGK60R6o-"},"source":["\n","class LSTMClassifier(pl.LightningModule):\n","\n","    def __init__(self, vocab_size, embedding_length, output_dim, hid_dim, dropout):\n","        super(LSTMClassifier, self).__init__()\n","\n","        self.attention = EM.SelfAttention(hid_dim, dropout)\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n","        self.lstm = nn.LSTM(embedding_length, hid_dim, bidirectional=True)\n","        self.hid_dim = hid_dim\n","\n","        self.drop_1 = nn.Dropout(dropout)\n","        self.drop_2 = nn.Dropout(dropout)\n","        self.pred = nn.Linear(hid_dim * 2, output_dim) ## output_dim -> 9\n","\n","    def forward(self, inp):\n","\n","        embedded_inp = self.drop_1(self.word_embeddings(inp)) ## embedded_inp.size() = (batch size, sequence length, embedding size)\n","\n","        embedded_inp = embedded_inp.permute(1, 0, 2) ## embedded_inp.size() = (sequence length, batch size, embedding size)\n","        \n","        h_0 = Variable(torch.zeros(2, embedded_inp.size(1), self.hid_dim)).cuda() ## h_0.size() = (num directions, batch size, hidden size)\n","        c_0 = Variable(torch.zeros(2, embedded_inp.size(1), self.hid_dim)).cuda() ## c_0.size() = (num directions, batch size, hidden size)\n","\n","        output, (final_hidden_state, final_cell_state) = self.lstm(embedded_inp, (h_0, c_0))\n","\n","        attention_product = self.attention(output, final_hidden_state)\n","\n","        pred = self.drop_2(self.pred(attention_product)) ## pred.size() = (batch size, output size)\n","\n","        return pred\n","\n","    def training_step(self, batch, batch_idx):\n","\n","        src, emotion = batch\n","\n","        output = self.forward(src)\n","\n","        loss = criterion(output.contiguous(), emotion)\n","\n","        return {'loss': loss}\n","\n","    def training_epoch_end(self, outputs):\n","\n","        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n","        print(\"Train loss: \", avg_loss)\n","        return {'train_loss': avg_loss}\n","\n","    def validation_step(self, batch, batch_idx):\n","\n","        src, emotion = batch\n","\n","        output = self.forward(src)\n","\n","        val_loss = criterion(output.contiguous(), emotion)\n","\n","        return {'val_loss': val_loss}\n","\n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        print(\"Avg loss: \", avg_loss)\n","        return {'val_loss': avg_loss}\n","\n","    def test_step(self, batch, batch_idx):\n","\n","        src, emotion = batch\n","\n","        output = self.forward(src)\n","\n","        test_loss = criterion(output.contiguous(), emotion)\n","\n","        return {'test_loss': test_loss}\n","\n","    def test_epoch_end(self, outputs):\n","\n","        test_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        return {'test_loss': test_loss}\n","\n","    def configure_optimizers(self):\n","\n","        return optim.Adam(model.parameters(), lr=0.0005)\n","\n","    def train_dataloader(self):\n","\n","        train_loader = DataLoader(train_dataset, batch_size=dpros2.batch_size, num_workers=3, drop_last=True, shuffle=True)\n","\n","        return train_loader\n","\n","    def val_dataloader(self):\n","\n","        valid_loader = DataLoader(valid_dataset, batch_size=dpros2.batch_size, num_workers=3, drop_last=True)\n","\n","        return valid_loader\n","\n","    def test_dataloader(self):\n","\n","        test_loader = DataLoader(test_dataset, batch_size=dpros2.batch_size, num_workers=3, drop_last=True)\n","\n","        return test_loader\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgvORJEH9AxB"},"source":["\n","INPUT_DIM = len(metadata['idx2w'])\n","OUTPUT_DIM = 9\n","EMB_DIM = 200\n","HID_DIM = 256\n","DROPOUT = 0.45\n","SEQ_LEN = dpros.limit['maxs']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbwiXRYMEavL"},"source":["\n","criterion = nn.CrossEntropyLoss().cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOCsKv_TfosD"},"source":["\n","## names of the emotions, for the confusion matrix\n","\n","names = ('anger','fear','joy','love','sadness','surprise','thankfulness','disgust','guilt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5L0oiyLc7FC"},"source":["\n","model = LSTMClassifier(INPUT_DIM, EMB_DIM, OUTPUT_DIM, HID_DIM, DROPOUT)\n","\n","checkpoint_callback = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Classifier/', save_top_k=1, verbose=True, monitor='val_loss',mode='min')\n","\n","trainer = pl.Trainer(gpus=1, max_epochs=100, log_save_interval=100000, weights_summary=None, progress_bar_refresh_rate=0, log_gpu_memory=None, benchmark=True, default_root_dir='drive/My Drive/Colab Notebooks/Classifier/', checkpoint_callback=checkpoint_callback)\n","\n","## trainer = pl.Trainer(resume_from_checkpoint='_ckpt_epoch_27.ckpt', gpus=1, max_epochs=28, log_save_interval=100000, weights_summary=None, progress_bar_refresh_rate=0, log_gpu_memory=None, benchmark=True, default_root_dir='drive/My Drive/Colab Notebooks/Classifier/', checkpoint_callback=checkpoint_callback)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw9J9Ahydk1a"},"source":["\n","trainer.fit(model)\n","trainer.test(model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaivSHSFlfvM"},"source":["\n","## this function outputs the emotion predictions of the model for different sentences\n","\n","def predict(model, src):\n","\n","    class_output = model(src)\n","    output_softmax = F.softmax(class_output, -1)\n","    predicted = torch.max(output_softmax.data, -1)\n","    predicted = predicted.indices\n","\n","    return predicted\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6rFabV4ek-o"},"source":["\n","## this is for making the predictions for the test set. Then, we store the predictions and target emotions in two lists\n","\n","test_loader = DataLoader(test_dataset, batch_size=dpros.BATCH_SIZE, drop_last=True)\n","\n","target = list()\n","predicted = list()\n","\n","model.freeze()\n","\n","for i, data in enumerate(test_loader, 0):\n","\n","    src, emotion = data\n","\n","    predicted_emotion = predict(model.cuda(), src.cuda())\n","\n","    for elem in predicted_emotion:\n","\n","      predicted.append(elem.cpu())\n","\n","    for elem2 in emotion:\n","\n","      target.append(elem2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEtJnjIQjoKm"},"source":["\n","## plot the confusion matrix for the test set\n","\n","print(classification_report(predicted, target, digits=10))\n","\n","cm = confusion_matrix(target, predicted)\n","\n","plt.figure(figsize=(10,10))\n","EM.plot_confusion_matrix(cm, names)\n"],"execution_count":null,"outputs":[]}]}