{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Final model.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"id":"vgyQxzatV2Vy"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjTEhDwdSMkU"},"source":["!pip install py-rouge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izZhCVf4qkh7","executionInfo":{"status":"ok","timestamp":1601474470786,"user_tz":-120,"elapsed":34475,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"d4fa68c3-8f96-4676-be14-b5097ab88a3d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"40GiOpWvFEyV"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"OPLXraeBE4xY","executionInfo":{"status":"ok","timestamp":1601474477539,"user_tz":-120,"elapsed":41221,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"13bd86c5-4e6a-4f95-a8e2-1bee372225ad","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import Data_processing as dpros\n","import Emotion_model as EM\n","import test_functions as test_func\n","import statistics\n","from tqdm import tqdm\n","from torch.autograd import Variable\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","import pytorch_lightning as pl\n","import nltk\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch.utils.data as utils\n","from torch.utils.data import Dataset, DataLoader\n","import nltk.translate.bleu_score as bleu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYASPjA0RFrG"},"source":["# Automatic Dialogue Generation with Expressed Emotions"]},{"cell_type":"markdown","metadata":{"id":"nz5vPHFo-Hwi"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"s-n8FRiUFE52"},"source":["\n","FILENAME = 'chat_emotion.txt'\n","\n","lines = dpros.read_lines(filename=FILENAME)\n","\n","# make every character lower case\n","lines = [ line.lower() for line in lines ]\n","\n","lines = [ dpros.filter_line(line, dpros.EN_WHITELIST) for line in lines ]\n","\n","qlines, asentence = dpros.filter_data(lines)\n","\n","alines = []\n","tag = []\n","\n","# separate the emotion from the sentence\n","for sentence in asentence:\n","    alines.append(sentence[:-2])\n","    tag.append(sentence[-1])\n","\n","qtokenized = [ wordlist.split(' ') for wordlist in qlines ]\n","atokenized = [ wordlist.split(' ') for wordlist in alines ]\n","\n","idx2w, w2idx = dpros.index_(qtokenized + atokenized, vocab_size=dpros.VOCAB_SIZE)\n","\n","idx_q, idx_a, q_length = dpros.zero_pad(qtokenized, atokenized, w2idx)\n","\n","metadata = {'w2idx': w2idx, 'idx2w': idx2w, 'limit': dpros.limit}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Tz2J8fMderK"},"source":["\n","word2id = metadata['w2idx']\n","id2word = metadata['idx2w']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_MepnmUHeOYf"},"source":["## Model 1: Enc-bef"]},{"cell_type":"code","metadata":{"id":"ZAPh-7wN7kKz"},"source":["\n","class Final_model_light(pl.LightningModule):\n","    \n","    def __init__(self, emb_dim, enc_hid_dim, hid_dim, vocab_size, num_directions, pad_len, encoder, decoder, dropout):\n","        \n","        super(Final_model_light, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n","\n","        self.encoder2decoder = nn.Linear(enc_hid_dim * num_directions, hid_dim)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hid_dim, vocab_size)\n","\n","    ## the forward method that makes the operations of the emotion embedding model and outputs the predictions\n","        \n","    def forward(self, input_sentence, input_length, target):\n","    \n","        input_sentence = self.embeddings(input_sentence)\n","        target = self.embeddings(target)\n","\n","        src, (src_hidden, src_cell) = self.encoder(input_sentence, input_length)\n","\n","        decoder_init_state = self.encoder2decoder(src_hidden)\n","\n","        decoder_init_state = self.tanh(decoder_init_state)\n","\n","        context = src.transpose(0, 1)\n","\n","        trg, (hidden_trg, cell_trg) = self.decoder(target, (decoder_init_state, src_cell), context)\n","\n","        logits = self.out(trg)\n","\n","        return logits\n","    \n","    ## updates the embedding weights with the pre-trained weights:\n","\n","    def load_word_embedding(self, id2word):\n","\n","        embeddings_index = {}\n","        f = open('drive/My Drive/Colab Notebooks/cc.en.300.vec', 'r', encoding='utf-8')\n","        for line in tqdm(f):\n","            values = line.rstrip().rsplit(' ')\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","        f.close()\n","\n","        embedding_matrix = np.zeros((vocab_size, emb_dim))\n","        for i, word in id2word.items():\n","            if i < vocab_size:\n","                embedding_vector = embeddings_index.get(word)\n","                if embedding_vector is not None:\n","                    embedding_matrix[i] = embedding_vector\n","                else:\n","                    if word == '<pad>':\n","                        embedding_matrix[i] = np.zeros([emb_dim])\n","                    else:\n","                        embedding_matrix[i] = np.random.uniform(-1, 1, emb_dim)\n","        self.embeddings.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n","        self.embeddings.requires_grad = False\n","\n","    ## definition of the training step:\n","    \n","    def training_step(self, batch, batch_idx):\n","        \n","        src, src_length, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        # makes the forward pass\n","\n","        decoder_logit = self.forward(src, src_length, trg_1)\n","\n","        ## calculate the loss\n","\n","        loss = criterion(decoder_logit.contiguous().view(-1, vocab_size), trg_2.contiguous().view(-1))\n","\n","        return {'loss': loss}\n","\n","    ## at the end of the training step, print the average loss\n","\n","    def training_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n","        print(\"Train loss: \", avg_loss)\n","        return {'training_loss': avg_loss}\n","    \n","    def validation_step(self, batch, batch_idx):\n","        \n","        src, src_length, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        # makes the forward pass\n","\n","        decoder_logit = self.forward(src, src_length, trg_1)\n","\n","        ## calculate the loss\n","\n","        val_loss = criterion(decoder_logit.contiguous().view(-1, vocab_size), trg_2.contiguous().view(-1))\n","\n","        return {'val_loss': val_loss}\n","\n","    ## at the end of the validation step, print the average loss\n","    \n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        print(\"Avg loss: \", avg_loss)\n","        return {'val_loss': avg_loss}\n","    \n","    def test_step(self, batch, batch_idx):\n","        \n","        src, src_length, trg = batch\n","\n","        trg_1 = trg[:, :-1]\n","        trg_2 = trg[:, 1:]\n","\n","        # makes the forward pass\n","\n","        decoder_logit = self.forward(src, src_length, trg_1)\n","\n","        # calculate the loss\n","\n","        test_loss = criterion(decoder_logit.contiguous().view(-1, vocab_size), trg_2.contiguous().view(-1))\n","\n","        return {'test_loss': test_loss}\n","\n","    ## at the end of the test step, print the average loss\n","    \n","    def test_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        return {'test_loss': avg_loss}\n","    \n","    def configure_optimizers(self):\n","        return optim.Adam(self.parameters(), lr=0.0001)\n","    \n","    def train_dataloader(self):\n","        \n","        train_loader = DataLoader(training_set, batch_size=dpros.batch_size, shuffle=True, num_workers=3)\n","        \n","        return train_loader\n","    \n","    def val_dataloader(self):\n","        \n","        valid_loader = DataLoader(valid_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return valid_loader\n","    \n","    def test_dataloader(self):\n","        \n","        test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3)\n","        \n","        return test_loader\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXuqvnHQXhCP"},"source":["### Train model"]},{"cell_type":"code","metadata":{"id":"Z_hEhyNCRWE3"},"source":["\n","## divide the sentences in train, test and validation. Then, add the emotion tokens to the source sentences and finally create the dataset objects for the three sets:\n","\n","test_index = int(0.05 * len(idx_q))\n","train_index = int(len(idx_q) - test_index)\n","valid_index = int(train_index - int(0.95 * train_index))\n","train_index = int(0.95 * train_index)\n","\n","X_train = [idx_q[i] for i in range(0, train_index)]\n","y_train = [idx_a[i] for i in range(0, train_index)]\n","length_train = [q_length[i] for i in range(0, train_index)]\n","tag_train = [tag[i] for i in range(0, train_index)]\n","\n","X_valid = [idx_q[i] for i in range(train_index + 1, train_index + valid_index)]\n","y_valid = [idx_a[i] for i in range(train_index + 1, train_index + valid_index)]\n","length_valid = [q_length[i] for i in range(train_index + 1, train_index + valid_index)]\n","tag_valid = [tag[i] for i in range(train_index + 1, train_index + valid_index)]\n","\n","X_test = [idx_q[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","y_test = [idx_a[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","length_test = [q_length[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","tag_test = [tag[i] for i in range(train_index + valid_index + 1, len(idx_q))]\n","\n","\n","X_train_emotion, length_trains = dpros.emotion_pad(X_train, length_train, tag_train, word2id)\n","X_valid_emotion, length_valids = dpros.emotion_pad(X_valid, length_valid, tag_valid, word2id)\n","X_test_emotion, length_tests = dpros.emotion_pad(X_test, length_test, tag_test, word2id)\n","\n","training_set = utils.TensorDataset(torch.LongTensor(X_train_emotion), torch.LongTensor(length_trains), torch.LongTensor(y_train))\n","valid_set = utils.TensorDataset(torch.LongTensor(X_valid_emotion), torch.LongTensor(length_valids), torch.LongTensor(y_valid))\n","test_set = utils.TensorDataset(torch.LongTensor(X_test_emotion), torch.LongTensor(length_tests), torch.LongTensor(y_test))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqGG3nKPSd70"},"source":["\n","input_dim = len(word2id)\n","pad_len = 30\n","emb_dim = 300\n","dec_hid_dim = 600\n","enc_hid_dim = 300\n","hid_dim = 600\n","dropout = 0.75\n","vocab_size = len(word2id)\n","target_pad = word2id['<pad>']\n","num_directions = 2\n","\n","encoder = EM.Encoder(input_dim, emb_dim, enc_hid_dim, dropout)\n","decoder = EM.Decoder(input_dim, emb_dim, dec_hid_dim, dropout)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJwJahpuoogv"},"source":["\n","criterion = nn.CrossEntropyLoss().cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WpwJv_YiV0z-"},"source":["\n","light_model = Final_model_light(emb_dim, enc_hid_dim, hid_dim, vocab_size, num_directions, pad_len, encoder, decoder, dropout)\n","light_model.load_word_embedding(id2word)\n","\n","checkpoint_callback = ModelCheckpoint(filepath='drive/My Drive/Colab Notebooks/Model_checkpoint/', save_top_k=1, verbose=True, monitor='val_loss',mode='min')\n","\n","trainer = pl.Trainer(gpus=1, max_epochs=3, log_save_interval=100000, weights_summary=None, log_gpu_memory=None, progress_bar_refresh_rate=0, default_root_dir='drive/My Drive/Colab Notebooks/Model_checkpoint/', checkpoint_callback=checkpoint_callback)\n","\n","## trainer = pl.Trainer(resume_from_checkpoint='drive/My Drive/Colab Notebooks/Model_checkpoint/epoch=19.ckpt', gpus=1, max_epochs=20, log_save_interval=100000, weights_summary=None, log_gpu_memory=None, progress_bar_refresh_rate=0, default_root_dir='drive/My Drive/Colab Notebooks/Model_checkpoint/', checkpoint_callback=checkpoint_callback)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IflpXhTU7kLA"},"source":["\n","trainer.fit(light_model)\n","trainer.test(light_model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa_4YUSl7kLT"},"source":["## Evaluation metrics"]},{"cell_type":"code","metadata":{"id":"0r06yC0P7kLW","executionInfo":{"status":"ok","timestamp":1601476717259,"user_tz":-120,"elapsed":418010,"user":{"displayName":"Ander M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYt_lEcIFUxO2Xq14uGQivj1SpRP4rwxnyROgi=s64","userId":"18123863128901631014"}},"outputId":"fa1c9826-ac93-4247-c74f-519e747b74dc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","\n","answers, predict = test_func.create_answers_preds(light_model, test_loader, word2id, id2word)\n","\n","print(\"Created successfully!\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Created successfully!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l1hC472FWsZD"},"source":["\n","for i in range(0, len(answers)):\n","\n","    answers[i] = answers[i].split('<pad>')[0]\n","    predict[i] = predict[i].split('<pad>')[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrHPby307kLa"},"source":["### BLEU"]},{"cell_type":"code","metadata":{"id":"Ai36h99NVUK8"},"source":["\n","test_func.bleu_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Z_1HMp67kLd"},"source":["### ROUGE"]},{"cell_type":"code","metadata":{"id":"h0sRVcuGWvSa"},"source":["\n","test_func.rouge_scores(answers, predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BEj2sxr7kLn"},"source":["### Embedding average metric"]},{"cell_type":"code","metadata":{"id":"1kCJsHB3WzxN"},"source":["\n","test_func.word_embedding_scores(answers, predict, light_model, word2id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXkyBqShmTfK"},"source":["\n","## this is for creating the predictions for each emotion and store them in separate files\n","\n","for tag in range(9):\n","    X_test_emotion, length_tests = dpros.emotion_pad(X_test, length_test, tag, word2id)\n","    test_set = utils.TensorDataset(torch.LongTensor(X_test_emotion), torch.LongTensor(length_tests), torch.LongTensor(y_test))\n","    test_loader = DataLoader(test_set, batch_size=dpros.batch_size, num_workers=3, drop_last=True)\n","    sources, predict = test_func.create_sources_preds(light_model, test_loader, word2id, id2word)\n","\n","    print(\"Emotion \" + str(tag))\n","\n","    df = pd.DataFrame({'Sources': [''.join(source_test) for source_test in sources], 'Predictions': [''.join(predicted_test) for predicted_test in predict]})\n","    df.to_csv('Emotion' + str(tag) + '.csv', encoding='utf-8', index=False)\n"],"execution_count":null,"outputs":[]}]}